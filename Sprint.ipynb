{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import sys, re\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "import theano.tensor.shared_randomstreams\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    y = T.maximum(0.0, x)\n",
    "    return(y)\n",
    "def Sigmoid(x):\n",
    "    y = T.nnet.sigmoid(x)\n",
    "    return(y)\n",
    "def Tanh(x):\n",
    "    y = T.tanh(x)\n",
    "    return(y)\n",
    "def Iden(x):\n",
    "    y = x\n",
    "    return(y)\n",
    "        \n",
    "class HiddenLayer(object):\n",
    "    \"\"\"\n",
    "    Class for HiddenLayer\n",
    "    \"\"\"\n",
    "    def __init__(self, rng, input, n_in, n_out, activation, W=None, b=None,\n",
    "                 use_bias=False):\n",
    "\n",
    "        self.input = input\n",
    "        self.activation = activation\n",
    "\n",
    "        if W is None:            \n",
    "            if activation.func_name == \"ReLU\":\n",
    "                W_values = np.asarray(0.01 * rng.standard_normal(size=(n_in, n_out)), dtype=theano.config.floatX)\n",
    "            else:                \n",
    "                W_values = np.asarray(rng.uniform(low=-np.sqrt(6. / (n_in + n_out)), high=np.sqrt(6. / (n_in + n_out)),\n",
    "                                                     size=(n_in, n_out)), dtype=theano.config.floatX)\n",
    "            W = theano.shared(value=W_values, name='W')        \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b')\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        if use_bias:\n",
    "            lin_output = T.dot(input, self.W) + self.b\n",
    "        else:\n",
    "            lin_output = T.dot(input, self.W)\n",
    "\n",
    "        self.output = (lin_output if activation is None else activation(lin_output))\n",
    "    \n",
    "        # parameters of the model\n",
    "        if use_bias:\n",
    "            self.params = [self.W, self.b]\n",
    "        else:\n",
    "            self.params = [self.W]\n",
    "\n",
    "def _dropout_from_layer(rng, layer, p):\n",
    "    \"\"\"p is the probablity of dropping a unit\n",
    "\"\"\"\n",
    "    srng = theano.tensor.shared_randomstreams.RandomStreams(rng.randint(999999))\n",
    "    # p=1-p because 1's indicate keep and p is prob of dropping\n",
    "    mask = srng.binomial(n=1, p=1-p, size=layer.shape)\n",
    "    # The cast is important because\n",
    "    # int * float32 = float64 which pulls things off the gpu\n",
    "    output = layer * T.cast(mask, theano.config.floatX)\n",
    "    return output\n",
    "\n",
    "class DropoutHiddenLayer(HiddenLayer):\n",
    "    def __init__(self, rng, input, n_in, n_out,\n",
    "                 activation, dropout_rate, use_bias, W=None, b=None):\n",
    "        super(DropoutHiddenLayer, self).__init__(\n",
    "                rng=rng, input=input, n_in=n_in, n_out=n_out, W=W, b=b,\n",
    "                activation=activation, use_bias=use_bias)\n",
    "\n",
    "        self.output = _dropout_from_layer(rng, self.output, p=dropout_rate)\n",
    "\n",
    "class MLPDropout(object):\n",
    "    \"\"\"A multilayer perceptron with dropout\"\"\"\n",
    "    def __init__(self,rng,input,layer_sizes,dropout_rates,activations,use_bias=True):\n",
    "\n",
    "        #rectified_linear_activation = lambda x: T.maximum(0.0, x)\n",
    "\n",
    "        # Set up all the hidden layers\n",
    "        self.weight_matrix_sizes = zip(layer_sizes, layer_sizes[1:])\n",
    "        self.layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.activations = activations\n",
    "        next_layer_input = input\n",
    "        #first_layer = True\n",
    "        # dropout the input\n",
    "        next_dropout_layer_input = _dropout_from_layer(rng, input, p=dropout_rates[0])\n",
    "        layer_counter = 0\n",
    "        for n_in, n_out in self.weight_matrix_sizes[:-1]:\n",
    "            next_dropout_layer = DropoutHiddenLayer(rng=rng,\n",
    "                    input=next_dropout_layer_input,\n",
    "                    activation=activations[layer_counter],\n",
    "                    n_in=n_in, n_out=n_out, use_bias=use_bias,\n",
    "                    dropout_rate=dropout_rates[layer_counter])\n",
    "            self.dropout_layers.append(next_dropout_layer)\n",
    "            next_dropout_layer_input = next_dropout_layer.output\n",
    "\n",
    "            # Reuse the parameters from the dropout layer here, in a different\n",
    "            # path through the graph.\n",
    "            next_layer = HiddenLayer(rng=rng,\n",
    "                    input=next_layer_input,\n",
    "                    activation=activations[layer_counter],\n",
    "                    # scale the weight matrix W with (1-p)\n",
    "                    W=next_dropout_layer.W * (1 - dropout_rates[layer_counter]),\n",
    "                    b=next_dropout_layer.b,\n",
    "                    n_in=n_in, n_out=n_out,\n",
    "                    use_bias=use_bias)\n",
    "            self.layers.append(next_layer)\n",
    "            next_layer_input = next_layer.output\n",
    "            #first_layer = False\n",
    "            layer_counter += 1\n",
    "        \n",
    "        # Set up the output layer\n",
    "        n_in, n_out = self.weight_matrix_sizes[-1]\n",
    "        dropout_output_layer = LogisticRegression(\n",
    "                input=next_dropout_layer_input,\n",
    "                n_in=n_in, n_out=n_out)\n",
    "        self.dropout_layers.append(dropout_output_layer)\n",
    "\n",
    "        # Again, reuse paramters in the dropout output.\n",
    "        output_layer = LogisticRegression(\n",
    "            input=next_layer_input,\n",
    "            # scale the weight matrix W with (1-p)\n",
    "            W=dropout_output_layer.W * (1 - dropout_rates[-1]),\n",
    "            b=dropout_output_layer.b,\n",
    "            n_in=n_in, n_out=n_out)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        # Use the negative log likelihood of the logistic regression layer as\n",
    "        # the objective.\n",
    "        self.dropout_negative_log_likelihood = self.dropout_layers[-1].negative_log_likelihood\n",
    "        self.dropout_errors = self.dropout_layers[-1].errors\n",
    "\n",
    "        self.negative_log_likelihood = self.layers[-1].negative_log_likelihood\n",
    "        self.errors = self.layers[-1].errors\n",
    "\n",
    "        # Grab all the parameters together.\n",
    "        self.params = [ param for layer in self.dropout_layers for param in layer.params ]\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        next_layer_input = new_data\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            if i<len(self.layers)-1:\n",
    "                next_layer_input = self.activations[i](T.dot(next_layer_input,layer.W) + layer.b)\n",
    "            else:\n",
    "                p_y_given_x = T.nnet.softmax(T.dot(next_layer_input, layer.W) + layer.b)\n",
    "        y_pred = T.argmax(p_y_given_x, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def predict_p(self, new_data):\n",
    "        next_layer_input = new_data\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            if i<len(self.layers)-1:\n",
    "                next_layer_input = self.activations[i](T.dot(next_layer_input,layer.W) + layer.b)\n",
    "            else:\n",
    "                p_y_given_x = T.nnet.softmax(T.dot(next_layer_input, layer.W) + layer.b)\n",
    "        return p_y_given_x\n",
    "        \n",
    "class MLP(object):\n",
    "    \"\"\"Multi-Layer Perceptron Class\n",
    "    A multilayer perceptron is a feedforward artificial neural network model\n",
    "    that has one layer or more of hidden units and nonlinear activations.\n",
    "    Intermediate layers usually have as activation function tanh or the\n",
    "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "    top layer is a softamx layer (defined here by a ``LogisticRegression``\n",
    "    class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "        \"\"\"\n",
    "\n",
    "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
    "        # into a HiddenLayer with a tanh activation function connected to the\n",
    "        # LogisticRegression layer; the activation function can be replaced by\n",
    "        # sigmoid or any other nonlinear function\n",
    "        self.hiddenLayer = HiddenLayer(rng=rng, input=input,\n",
    "                                       n_in=n_in, n_out=n_hidden,\n",
    "                                       activation=T.tanh)\n",
    "\n",
    "        # The logistic regression layer gets as input the hidden units\n",
    "        # of the hidden layer\n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out)\n",
    "\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "\n",
    "        # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood\n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "\n",
    "        # the parameters of the model are the parameters of the two layer it is\n",
    "        # made out of\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        \n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "    :type input: theano.tensor.TensorType\n",
    "    :param input: symbolic variable that describes the input of the\n",
    "    architecture (one minibatch)\n",
    "    \n",
    "    :type n_in: int\n",
    "    :param n_in: number of input units, the dimension of the space in\n",
    "    which the datapoints lie\n",
    "    \n",
    "    :type n_out: int\n",
    "    :param n_out: number of output units, the dimension of the space in\n",
    "    which the labels lie\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        if W is None:\n",
    "            self.W = theano.shared(\n",
    "                    value=np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
    "                    name='W')\n",
    "        else:\n",
    "            self.W = W\n",
    "\n",
    "        # initialize the baises b as a vector of n_out 0s\n",
    "        if b is None:\n",
    "            self.b = theano.shared(\n",
    "                    value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "                    name='b')\n",
    "        else:\n",
    "            self.b = b\n",
    "\n",
    "        # compute vector of class-membership probabilities in symbolic form\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # compute prediction as class whose probability is maximal in\n",
    "        # symbolic form\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "    .. math::\n",
    "    \n",
    "    \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "    \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "    \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "    \n",
    "    :type y: theano.tensor.TensorType\n",
    "    :param y: corresponds to a vector that gives for each example the\n",
    "    correct label\n",
    "    \n",
    "    Note: we use the mean instead of the sum so that\n",
    "    the learning rate is less dependent on the batch size\n",
    "    \"\"\"\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch ;\n",
    "    zero one loss over the size of the minibatch\n",
    "    \n",
    "    :type y: theano.tensor.TensorType\n",
    "    :param y: corresponds to a vector that gives for each example the\n",
    "    correct label\n",
    "    \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_pred',\n",
    "                ('y', target.type, 'y_pred', self.y_pred.type))\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "class LeNetConvPoolLayer(object):\n",
    "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2), non_linear=\"tanh\"):\n",
    "        \"\"\"\n",
    "        Allocate a LeNetConvPoolLayer with shared variable internal parameters.\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "        :type input: theano.tensor.dtensor4\n",
    "        :param input: symbolic image tensor, of shape image_shape\n",
    "        :type filter_shape: tuple or list of length 4\n",
    "        :param filter_shape: (number of filters, num input feature maps,\n",
    "                              filter height,filter width)\n",
    "        :type image_shape: tuple or list of length 4\n",
    "        :param image_shape: (batch size, num input feature maps,\n",
    "                             image height, image width)\n",
    "        :type poolsize: tuple or list of length 2\n",
    "        :param poolsize: the downsampling (pooling) factor (#rows,#cols)\n",
    "        \"\"\"\n",
    "\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "        self.input = input\n",
    "        self.filter_shape = filter_shape\n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.non_linear = non_linear\n",
    "        # there are \"num input feature maps * filter height * filter width\"\n",
    "        # inputs to each hidden unit\n",
    "        fan_in = np.prod(filter_shape[1:])\n",
    "        # each unit in the lower layer receives a gradient from:\n",
    "        # \"num output feature maps * filter height * filter width\" /\n",
    "        #   pooling size\n",
    "        fan_out = (filter_shape[0] * np.prod(filter_shape[2:]) /np.prod(poolsize))\n",
    "        # initialize weights with random weights\n",
    "        if self.non_linear==\"none\" or self.non_linear==\"relu\":\n",
    "            self.W = theano.shared(np.asarray(rng.uniform(low=-0.01,high=0.01,size=filter_shape), \n",
    "                                                dtype=theano.config.floatX),borrow=True,name=\"W_conv\")\n",
    "        else:\n",
    "            W_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "            self.W = theano.shared(np.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX),borrow=True,name=\"W_conv\")   \n",
    "        b_values = np.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True, name=\"b_conv\")\n",
    "        \n",
    "        # convolve input feature maps with filters\n",
    "        conv_out = conv.conv2d(input=input, filters=self.W,filter_shape=self.filter_shape, image_shape=self.image_shape)\n",
    "        if self.non_linear==\"tanh\":\n",
    "            conv_out_tanh = T.tanh(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "            self.output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
    "        elif self.non_linear==\"relu\":\n",
    "            conv_out_tanh = ReLU(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "            self.output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
    "        else:\n",
    "            pooled_out = downsample.max_pool_2d(input=conv_out, ds=self.poolsize, ignore_border=True)\n",
    "            self.output = pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def predict(self, new_data, batch_size):\n",
    "        \"\"\"\n",
    "        predict for new data\n",
    "        \"\"\"\n",
    "        img_shape = (batch_size, 1, self.image_shape[2], self.image_shape[3])\n",
    "        conv_out = conv.conv2d(input=new_data, filters=self.W, filter_shape=self.filter_shape, image_shape=img_shape)\n",
    "        if self.non_linear==\"tanh\":\n",
    "            conv_out_tanh = T.tanh(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "            output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
    "        if self.non_linear==\"relu\":\n",
    "            conv_out_tanh = ReLU(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "            output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
    "        else:\n",
    "            pooled_out = downsample.max_pool_2d(input=conv_out, ds=self.poolsize, ignore_border=True)\n",
    "            output = pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_data_cv(X,y, cv=10, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into 10 folds.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    s_vocab = defaultdict(float)\n",
    "    k=0\n",
    "    for i in X:\n",
    "        sentences = []\n",
    "        for j in i:\n",
    "            rev = []\n",
    "            rev.append(clean_str(j).strip())\n",
    "            orig_rev = \" \".join(rev)\n",
    "            words = set(orig_rev.split())\n",
    "            sentences.append(orig_rev)\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            for s in sentences:\n",
    "                s_vocab[s] += 1\n",
    "        datum  = {\"y\":y[k], \n",
    "                      \"text\": sentences,\n",
    "                      \"num_words\": len(str(sentences).split()),\n",
    "                      \"split\": np.random.randint(0,cv)}\n",
    "        k += 1\n",
    "    revs.append(datum)\n",
    "    return revs, vocab,s_vocab\n",
    "    \n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype='float32')            \n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def get_S_W(s_vocab,W,word_idx_map,max_l,l=300):\n",
    "    s_vocab_size = len(s_vocab)\n",
    "    sentence_idx_map = dict()\n",
    "    S = np.zeros(shape=(s_vocab_size+1, 500000), dtype='float32')            \n",
    "    S[0] = np.zeros(500000, dtype='float32')\n",
    "    k=1\n",
    "    for i in s_vocab:\n",
    "        #print \"i\",i\n",
    "        s_W = np.array([])\n",
    "        for j in i.split(\" \"):\n",
    "            try:\n",
    "                s_W=np.concatenate([s_W,np.array(W[word_idx_map[j]])])\n",
    "            except:\n",
    "                continue\n",
    "        S[k] = np.pad(s_W,(0,500000-len(s_W)),'constant')\n",
    "        sentence_idx_map[i] = k\n",
    "        k +=1\n",
    "    return S, sentence_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip()# if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_conv_net(datasets,\n",
    "                   U,\n",
    "                   img_w=300, \n",
    "                   filter_hs=[3,4,5],\n",
    "                   hidden_units=[100,2], \n",
    "                   dropout_rate=[0.5],\n",
    "                   shuffle_batch=True,\n",
    "                   n_epochs=25, \n",
    "                   batch_size=50, \n",
    "                   lr_decay = 0.95,\n",
    "                   conv_non_linear=\"relu\",\n",
    "                   activations=[Iden],\n",
    "                   sqr_norm_lim=9,\n",
    "                   non_static=True):\n",
    "    \"\"\"\n",
    "    Train a simple conv net\n",
    "    img_h = sentence length (padded where necessary)\n",
    "    img_w = word vector length (300 for word2vec)\n",
    "    filter_hs = filter window sizes    \n",
    "    hidden_units = [x,y] x is the number of feature maps (per filter window), and y is the penultimate layer\n",
    "    sqr_norm_lim = s^2 in the paper\n",
    "    lr_decay = adadelta decay parameter\n",
    "    \"\"\"    \n",
    "    rng = np.random.RandomState(3435)\n",
    "    img_h = len(datasets[0][0])-1  \n",
    "    filter_w = img_w    \n",
    "    feature_maps = hidden_units[0]\n",
    "    filter_shapes = []\n",
    "    pool_sizes = []\n",
    "    for filter_h in filter_hs:\n",
    "        filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "        pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n",
    "    parameters = [(\"image shape\",img_h,img_w),(\"filter shape\",filter_shapes), (\"hidden_units\",hidden_units),\n",
    "                  (\"dropout\", dropout_rate), (\"batch_size\",batch_size),(\"non_static\", non_static),\n",
    "                    (\"learn_decay\",lr_decay), (\"conv_non_linear\", conv_non_linear), (\"non_static\", non_static)\n",
    "                    ,(\"sqr_norm_lim\",sqr_norm_lim),(\"shuffle_batch\",shuffle_batch)]\n",
    "    print parameters    \n",
    "    \n",
    "    #define model architecture\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')   \n",
    "    y = T.ivector('y')\n",
    "    Words = theano.shared(value = U, name = \"Words\")\n",
    "    zero_vec_tensor = T.vector()\n",
    "    zero_vec = np.zeros(img_w)\n",
    "    set_zero = theano.function([zero_vec_tensor], updates=[(Words, T.set_subtensor(Words[0,:], zero_vec_tensor))], allow_input_downcast=True)\n",
    "    layer0_input = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((x.shape[0],1,x.shape[1],Words.shape[1]))                                  \n",
    "    conv_layers = []\n",
    "    layer1_inputs = []\n",
    "    for i in xrange(len(filter_hs)):\n",
    "        filter_shape = filter_shapes[i]\n",
    "        pool_size = pool_sizes[i]\n",
    "        conv_layer = LeNetConvPoolLayer(rng, input=layer0_input,image_shape=(batch_size, 1, img_h, img_w),\n",
    "                                filter_shape=filter_shape, poolsize=pool_size, non_linear=conv_non_linear)\n",
    "        layer1_input = conv_layer.output.flatten(2)\n",
    "        conv_layers.append(conv_layer)\n",
    "        layer1_inputs.append(layer1_input)\n",
    "    layer1_input = T.concatenate(layer1_inputs,1)\n",
    "    hidden_units[0] = feature_maps*len(filter_hs)    \n",
    "    classifier = MLPDropout(rng, input=layer1_input, layer_sizes=hidden_units, activations=activations, dropout_rates=dropout_rate)\n",
    "    \n",
    "    #define parameters of the model and update functions using adadelta\n",
    "    params = classifier.params     \n",
    "    for conv_layer in conv_layers:\n",
    "        params += conv_layer.params\n",
    "    if non_static:\n",
    "        #if word vectors are allowed to change, add them as model parameters\n",
    "        params += [Words]\n",
    "    cost = classifier.negative_log_likelihood(y) \n",
    "    dropout_cost = classifier.dropout_negative_log_likelihood(y)           \n",
    "    grad_updates = sgd_updates_adadelta(params, dropout_cost, lr_decay, 1e-6, sqr_norm_lim)\n",
    "    \n",
    "    #shuffle dataset and assign to mini batches. if dataset size is not a multiple of mini batches, replicate \n",
    "    #extra data (at random)\n",
    "    np.random.seed(3435)\n",
    "    if datasets[0].shape[0] % batch_size > 0:\n",
    "        extra_data_num = batch_size - datasets[0].shape[0] % batch_size\n",
    "        train_set = np.random.permutation(datasets[0])   \n",
    "        extra_data = train_set[:extra_data_num]\n",
    "        new_data=np.append(datasets[0],extra_data,axis=0)\n",
    "    else:\n",
    "        new_data = datasets[0]\n",
    "    new_data = np.random.permutation(new_data)\n",
    "    n_batches = new_data.shape[0]/batch_size\n",
    "    n_train_batches = int(np.round(n_batches*0.9))\n",
    "    #divide train set into train/val sets \n",
    "    test_set_x = datasets[1][:,:img_h] \n",
    "    test_set_y = np.asarray(datasets[1][:,-1],\"int32\")\n",
    "    train_set = new_data[:n_train_batches*batch_size,:]\n",
    "    val_set = new_data[n_train_batches*batch_size:,:]     \n",
    "    train_set_x, train_set_y = shared_dataset((train_set[:,:img_h],train_set[:,-1]))\n",
    "    val_set_x, val_set_y = shared_dataset((val_set[:,:img_h],val_set[:,-1]))\n",
    "    n_val_batches = n_batches - n_train_batches\n",
    "    val_model = theano.function([index], classifier.errors(y),\n",
    "         givens={\n",
    "            x: val_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "             y: val_set_y[index * batch_size: (index + 1) * batch_size]},\n",
    "                                allow_input_downcast=True)\n",
    "            \n",
    "    #compile theano functions to get train/val/test errors\n",
    "    test_model = theano.function([index], classifier.errors(y),\n",
    "             givens={\n",
    "                x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                 y: train_set_y[index * batch_size: (index + 1) * batch_size]},\n",
    "                                 allow_input_downcast=True)               \n",
    "    train_model = theano.function([index], cost, updates=grad_updates,\n",
    "          givens={\n",
    "            x: train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "              y: train_set_y[index*batch_size:(index+1)*batch_size]},\n",
    "                                  allow_input_downcast = True)     \n",
    "    test_pred_layers = []\n",
    "    test_size = test_set_x.shape[0]\n",
    "    test_layer0_input = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((test_size,1,img_h,Words.shape[1]))\n",
    "    for conv_layer in conv_layers:\n",
    "        test_layer0_output = conv_layer.predict(test_layer0_input, test_size)\n",
    "        test_pred_layers.append(test_layer0_output.flatten(2))\n",
    "    test_layer1_input = T.concatenate(test_pred_layers, 1)\n",
    "    test_y_pred = classifier.predict(test_layer1_input)\n",
    "    test_error = T.mean(T.neq(test_y_pred, y))\n",
    "    test_model_all = theano.function([x,y], test_error, allow_input_downcast = True)   \n",
    "    \n",
    "    #start training over mini-batches\n",
    "    print '... training'\n",
    "    epoch = 0\n",
    "    best_val_perf = 0\n",
    "    val_perf = 0\n",
    "    test_perf = 0       \n",
    "    cost_epoch = 0    \n",
    "    while (epoch < n_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch = epoch + 1\n",
    "        if shuffle_batch:\n",
    "            for minibatch_index in np.random.permutation(range(n_train_batches)):\n",
    "                cost_epoch = train_model(minibatch_index)\n",
    "                set_zero(zero_vec)\n",
    "        else:\n",
    "            for minibatch_index in xrange(n_train_batches):\n",
    "                cost_epoch = train_model(minibatch_index)  \n",
    "                set_zero(zero_vec)\n",
    "        train_losses = [test_model(i) for i in xrange(n_train_batches)]\n",
    "        train_perf = 1 - np.mean(train_losses)\n",
    "        val_losses = [val_model(i) for i in xrange(n_val_batches)]\n",
    "        val_perf = 1- np.mean(val_losses)                        \n",
    "        print('epoch: %i, training time: %.2f secs, train perf: %.2f %%, val perf: %.2f %%' % (epoch, time.time()-start_time, train_perf * 100., val_perf*100.))\n",
    "        if val_perf >= best_val_perf:\n",
    "            best_val_perf = val_perf\n",
    "            test_loss = test_model_all(test_set_x,test_set_y)        \n",
    "            test_perf = 1- test_loss\n",
    "    return test_perf, classifier\n",
    "\n",
    "def predict(x,classifier):\n",
    "    return theano.function([index], classifier.y_pred,\n",
    "                   givens={x: x})\n",
    "\n",
    "def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "        \n",
    "def sgd_updates_adadelta(params,cost,rho=0.95,epsilon=1e-6,norm_lim=9,word_vec_name='Words'):\n",
    "    \"\"\"\n",
    "    adadelta update rule, mostly from\n",
    "    https://groups.google.com/forum/#!topic/pylearn-dev/3QbKtCumAW4 (for Adadelta)\n",
    "    \"\"\"\n",
    "    updates = OrderedDict({})\n",
    "    exp_sqr_grads = OrderedDict({})\n",
    "    exp_sqr_ups = OrderedDict({})\n",
    "    gparams = []\n",
    "    for param in params:\n",
    "        empty = np.zeros_like(param.get_value())\n",
    "        exp_sqr_grads[param] = theano.shared(value=as_floatX(empty),name=\"exp_grad_%s\" % param.name)\n",
    "        gp = T.grad(cost, param)\n",
    "        exp_sqr_ups[param] = theano.shared(value=as_floatX(empty), name=\"exp_grad_%s\" % param.name)\n",
    "        gparams.append(gp)\n",
    "    for param, gp in zip(params, gparams):\n",
    "        exp_sg = exp_sqr_grads[param]\n",
    "        exp_su = exp_sqr_ups[param]\n",
    "        up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp)\n",
    "        updates[exp_sg] = up_exp_sg\n",
    "        step =  -(T.sqrt(exp_su + epsilon) / T.sqrt(up_exp_sg + epsilon)) * gp\n",
    "        updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step)\n",
    "        stepped_param = param + step\n",
    "        if (param.get_value(borrow=True).ndim == 2) and (param.name!='Words'):\n",
    "            col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))\n",
    "            desired_norms = T.clip(col_norms, 0, T.sqrt(norm_lim))\n",
    "            scale = desired_norms / (1e-7 + col_norms)\n",
    "            updates[param] = stepped_param * scale\n",
    "        else:\n",
    "            updates[param] = stepped_param      \n",
    "    return updates \n",
    "\n",
    "def as_floatX(variable):\n",
    "    if isinstance(variable, float):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "\n",
    "    if isinstance(variable, np.ndarray):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "    return theano.tensor.cast(variable, theano.config.floatX)\n",
    "    \n",
    "def safe_update(dict_to, dict_from):\n",
    "    \"\"\"\n",
    "    re-make update dictionary for safe updating\n",
    "    \"\"\"\n",
    "    for key, val in dict(dict_from).iteritems():\n",
    "        if key in dict_to:\n",
    "            raise KeyError(key)\n",
    "        dict_to[key] = val\n",
    "    return dict_to\n",
    "    \n",
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for j in sent:\n",
    "        for i in xrange(pad):\n",
    "            x.append(0)\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            if word in word_idx_map:\n",
    "                x.append(word_idx_map[word])\n",
    "        while len(x) < max_l+2*pad:\n",
    "            x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv(revs, sentence_idx_map, cv, max_l=100000, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h)   \n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]==cv:            \n",
    "            test.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)   \n",
    "    train = np.array(train,dtype=\"int\")\n",
    "    test = np.array(test,dtype=\"int\")\n",
    "    return [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 1\n",
      "vocab size: 17393\n",
      "max sentence length: 39\n",
      "loading word2vec vectors... word2vec loaded!\n",
      "num words already in word2vec: 14987\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-5eb0e4e46d61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_idx_data_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrevs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_idx_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m56\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         perf,c = train_conv_net(datasets,\n\u001b[0;32m     37\u001b[0m                               \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-cbc606fcae99>\u001b[0m in \u001b[0;36mmake_idx_data_cv\u001b[1;34m(revs, word_idx_map, cv, max_l, k, filter_h)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrevs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_idx_from_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_idx_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"split\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-cbc606fcae99>\u001b[0m in \u001b[0;36mget_idx_from_sent\u001b[1;34m(sent, word_idx_map, max_l, k, filter_h)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_idx_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":    \n",
    "    w2v_file = \"E:\\\\GoogleNews-vectors-negative300.bin\"\n",
    "    \n",
    "    file=pd.read_csv(\".\\\\Documents\\\\test.csv\")\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for i in set(file['author']):\n",
    "        v=0\n",
    "        x=[j for j in file['body'][file['author']==i]]\n",
    "        for j in file['controversiality'][file['author']==i]:\n",
    "            if v<=1: v+=j\n",
    "            else: v=1\n",
    "        X.append(x)\n",
    "        y.append(v)\n",
    "    \n",
    "    revs, vocab, s_vocab = build_data_cv(X,y, cv=2, clean_string=False)\n",
    "    \n",
    "    max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
    "    print \"data loaded!\"\n",
    "    print \"number of sentences: \" + str(len(revs))\n",
    "    print \"vocab size: \" + str(len(vocab))\n",
    "    print \"max sentence length: \" + str(max_l)\n",
    "    print \"loading word2vec vectors...\",\n",
    "    w2v = load_bin_vec(w2v_file, vocab)\n",
    "    print \"word2vec loaded!\"\n",
    "    print \"num words already in word2vec: \" + str(len(w2v))\n",
    "    W, word_idx_map = get_W(w2v)\n",
    "    S, sentence_idx_map = get_S_W(s_vocab,W,word_idx_map,max_l)\n",
    "    \n",
    "    non_static=True \n",
    "    U = S\n",
    "    results = []\n",
    "    r = range(0,2)    \n",
    "    for i in r:\n",
    "        datasets = make_idx_data_cv(revs, sentence_idx_map, i, max_l=56,k=10000, filter_h=5)\n",
    "        perf,c = train_conv_net(datasets,\n",
    "                              U,\n",
    "                              lr_decay=0.95,\n",
    "                              filter_hs=[3,4,5],\n",
    "                              conv_non_linear=\"relu\",\n",
    "                              hidden_units=[100,2], \n",
    "                              shuffle_batch=True, \n",
    "                              n_epochs=3, \n",
    "                              sqr_norm_lim=9,\n",
    "                              non_static=non_static,\n",
    "                              batch_size=50,\n",
    "                              dropout_rate=[0.5])\n",
    "        print \"cv: \" + str(i) + \", perf: \" + str(perf)\n",
    "        results.append(perf)  \n",
    "    print str(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7c92ff69920a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello i like'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "predict('hello i like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
